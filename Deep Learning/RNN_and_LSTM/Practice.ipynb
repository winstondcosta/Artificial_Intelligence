{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fce7f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "279f7996",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, output_size, n_layers):\n",
    "        super(RNN,self).__init__()\n",
    "            \n",
    "        self.hidden = hidden_dim\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "            \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "            \n",
    "    def forward(self, x, hidden):\n",
    "        r_out, hidden = self.rnn(x, hidden)\n",
    "        r_out = r_out.view(-1, self.hidden)\n",
    "        output = self.fc(r_out)\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2d89681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size:  torch.Size([1, 20, 1])\n",
      "Output size:  torch.Size([20, 1])\n",
      "Hidden state size:  torch.Size([1, 1, 32])\n"
     ]
    }
   ],
   "source": [
    "model = RNN(input_size=1,hidden_dim=32, output_size=1, n_layers=1)\n",
    "\n",
    "seq_length = 20\n",
    "\n",
    "# generate evenly spaced, test data pts\n",
    "time_steps = np.linspace(0, np.pi, seq_length)\n",
    "data = np.sin(time_steps)\n",
    "data.resize((seq_length, 1))\n",
    "\n",
    "test_input = torch.Tensor(data).unsqueeze(0) # give it a batch_size of 1 as first dimension\n",
    "print('Input size: ', test_input.size())\n",
    "\n",
    "# test out rnn sizes\n",
    "test_out, test_h = model(test_input, None)\n",
    "print('Output size: ', test_out.size())\n",
    "print('Hidden state size: ', test_h.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9c0c872",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9577c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:  0.5435460209846497\n",
      "Training loss:  0.4270239770412445\n",
      "Training loss:  0.5434108376502991\n",
      "Training loss:  0.4270240366458893\n",
      "Training loss:  0.5434108376502991\n",
      "Training loss:  0.4270239770412445\n",
      "Training loss:  0.5434108376502991\n",
      "Training loss:  0.4270240366458893\n",
      "Training loss:  0.5434108376502991\n",
      "Training loss:  0.4270239770412445\n",
      "Training loss:  0.5434108376502991\n",
      "Training loss:  0.4270240366458893\n",
      "Training loss:  0.5434108376502991\n",
      "Training loss:  0.4270239770412445\n",
      "Training loss:  0.5434108376502991\n",
      "Training loss:  0.4270239770412445\n",
      "Training loss:  0.5434108376502991\n",
      "Training loss:  0.4270239770412445\n",
      "Training loss:  0.5434108376502991\n",
      "Training loss:  0.4270240366458893\n",
      "Training loss:  0.5434108376502991\n",
      "Training loss:  0.4270240366458893\n",
      "Training loss:  0.5434108376502991\n",
      "Training loss:  0.4270239770412445\n",
      "Training loss:  0.5434108376502991\n",
      "Training loss:  0.4270240366458893\n",
      "Training loss:  0.5434108376502991\n",
      "Training loss:  0.4270239770412445\n",
      "Training loss:  0.5434108376502991\n",
      "Training loss:  0.4270239770412445\n"
     ]
    }
   ],
   "source": [
    "n_steps = 30\n",
    "hidden = None\n",
    "for batch_i, step in enumerate(range(n_steps)):\n",
    "    # defining the training data \n",
    "    time_steps = np.linspace(step * np.pi, (step+1)*np.pi, seq_length + 1)\n",
    "    data = np.sin(time_steps)\n",
    "    data.resize((seq_length + 1, 1)) # input_size=1\n",
    "\n",
    "    x = data[:-1]\n",
    "    y = data[1:]\n",
    "        \n",
    "    # convert data into Tensors\n",
    "    x_tensor = torch.Tensor(x).unsqueeze(0) # unsqueeze gives a 1, batch_size dimension\n",
    "    y_tensor = torch.Tensor(y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pred, hidden = model(x_tensor, hidden)\n",
    "    \n",
    "    hidden = hidden.data\n",
    "    \n",
    "    loss = criterion(pred, y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"Training loss: \", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79706af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, tokens, embedding_dim, hidden_dim, output_size, n_layers, drop_prob):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.chars = tokens\n",
    "        int2char = dict(enumerate(self.chars))\n",
    "        char2int = {ch:i for i,ch in int2char.items()}\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, len(self.chars))\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        lstm_out = lstm_out[:,-1,:]\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        lstm_out = lstm_out.contiguous.view(-1, self.hidden_dim)\n",
    "        output = self.fc(lstm_out)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weights = next(self.parameters()).data\n",
    "        hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_(), \n",
    "                  weights.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0393265",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(chars, hidden_dim=32, n_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526f1f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    \n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    \n",
    "    val_idx = len(data)*val_split\n",
    "    train_data, val_data = data[val_idx:], data[:val_idx]\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for x, y in get_batches(train_data, batch_size, seq_length):\n",
    "        one_hot = one_hot_encode(x, len(model.chars))\n",
    "        inputs, labels = torch.from_numpy(one_hot), torch.from_numpy(y)\n",
    "        \n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, hidden = model(inputs, hidden)\n",
    "        \n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        val_hidden = model.init_hidden(batch_size)\n",
    "        val_losses = []\n",
    "        model.eval()\n",
    "        \n",
    "        for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "            one_hot = one_hot_encode(x, len(model.chars))\n",
    "            inputs, labels = torch.from_numpy(one_hot), torch.from_numpy(y)\n",
    "            \n",
    "            val_hidden = tuple([each.data for each in val_hidden])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output, val_hidden = model(inputs, val_hidden)\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            print(\"Validation loss: \", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c537854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, char, h, topk=None):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    x = np.array(model.char2int[char])\n",
    "    x = one_hot_encode(x, len(model.chars))\n",
    "    inputs = torch.from_numpy(x)\n",
    "    \n",
    "    h = tuple([each.data for each in h])\n",
    "    \n",
    "    out, h = model(inputs, h)\n",
    "    \n",
    "    prob = F.softmax(out, dim=1).data\n",
    "    \n",
    "    p, ch = prob.topk(topk)\n",
    "    ch = ch.numpy().squeeze()\n",
    "    p = p.numpy().squeeze()\n",
    "    \n",
    "    ch = np.random.choice(ch,p=p/p.sum())\n",
    "    \n",
    "    return int2char[ch], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc908d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, size, prime, topk=None):\n",
    "    \n",
    "    chars = [ch for ch in prime]\n",
    "    h = mode.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        ch, h = predict(model, ch, h, topk)\n",
    "    chars.append(ch)\n",
    "    \n",
    "    for i in range(size):\n",
    "        ch, h = predict(model, chars[-1], topk)\n",
    "        chars.append(ch)\n",
    "        \n",
    "    return \"\".join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d06d31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
