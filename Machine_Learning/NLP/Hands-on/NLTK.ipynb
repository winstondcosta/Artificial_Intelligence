{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b102d079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dfbe609",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/cirrusrays/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a17d4883",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Machine learning is a branch of artificial intelligence (AI) and computer science which focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b48c5d4",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c783ce20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine',\n",
       " 'learning',\n",
       " 'is',\n",
       " 'a',\n",
       " 'branch',\n",
       " 'of',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " '(',\n",
       " 'AI',\n",
       " ')',\n",
       " 'and',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'which',\n",
       " 'focuses',\n",
       " 'on',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'data',\n",
       " 'and',\n",
       " 'algorithms',\n",
       " 'to',\n",
       " 'imitate',\n",
       " 'the',\n",
       " 'way',\n",
       " 'that',\n",
       " 'humans',\n",
       " 'learn',\n",
       " ',',\n",
       " 'gradually',\n",
       " 'improving',\n",
       " 'its',\n",
       " 'accuracy',\n",
       " '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bba63d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Machine', 'learning', 'is'),\n",
       " ('learning', 'is', 'a'),\n",
       " ('is', 'a', 'branch'),\n",
       " ('a', 'branch', 'of'),\n",
       " ('branch', 'of', 'artificial'),\n",
       " ('of', 'artificial', 'intelligence'),\n",
       " ('artificial', 'intelligence', '('),\n",
       " ('intelligence', '(', 'AI'),\n",
       " ('(', 'AI', ')'),\n",
       " ('AI', ')', 'and'),\n",
       " (')', 'and', 'computer'),\n",
       " ('and', 'computer', 'science'),\n",
       " ('computer', 'science', 'which'),\n",
       " ('science', 'which', 'focuses'),\n",
       " ('which', 'focuses', 'on'),\n",
       " ('focuses', 'on', 'the'),\n",
       " ('on', 'the', 'use'),\n",
       " ('the', 'use', 'of'),\n",
       " ('use', 'of', 'data'),\n",
       " ('of', 'data', 'and'),\n",
       " ('data', 'and', 'algorithms'),\n",
       " ('and', 'algorithms', 'to'),\n",
       " ('algorithms', 'to', 'imitate'),\n",
       " ('to', 'imitate', 'the'),\n",
       " ('imitate', 'the', 'way'),\n",
       " ('the', 'way', 'that'),\n",
       " ('way', 'that', 'humans'),\n",
       " ('that', 'humans', 'learn'),\n",
       " ('humans', 'learn', ','),\n",
       " ('learn', ',', 'gradually'),\n",
       " (',', 'gradually', 'improving'),\n",
       " ('gradually', 'improving', 'its'),\n",
       " ('improving', 'its', 'accuracy'),\n",
       " ('its', 'accuracy', '.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bigrams, trigrams\n",
    "list(nltk.bigrams(tokens))\n",
    "list(nltk.trigrams(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b051af2",
   "metadata": {},
   "source": [
    "### POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "243a9bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Machine', 'NN')]\n",
      "[('learning', 'VBG')]\n",
      "[('is', 'VBZ')]\n",
      "[('a', 'DT')]\n",
      "[('branch', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('artificial', 'JJ')]\n",
      "[('intelligence', 'NN')]\n",
      "[('(', '(')]\n",
      "[('AI', 'NN')]\n",
      "[(')', ')')]\n",
      "[('and', 'CC')]\n",
      "[('computer', 'NN')]\n",
      "[('science', 'NN')]\n",
      "[('which', 'WDT')]\n",
      "[('focuses', 'NNS')]\n",
      "[('on', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('use', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('data', 'NNS')]\n",
      "[('and', 'CC')]\n",
      "[('algorithms', 'NN')]\n",
      "[('to', 'TO')]\n",
      "[('imitate', 'NN')]\n",
      "[('the', 'DT')]\n",
      "[('way', 'NN')]\n",
      "[('that', 'IN')]\n",
      "[('humans', 'NNS')]\n",
      "[('learn', 'NN')]\n",
      "[(',', ',')]\n",
      "[('gradually', 'RB')]\n",
      "[('improving', 'VBG')]\n",
      "[('its', 'PRP$')]\n",
      "[('accuracy', 'NN')]\n",
      "[('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/cirrusrays/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "for t in tokens:\n",
    "    print(nltk.pos_tag([t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aba1460e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Jerry', 'NN')]\n",
      "[(' ', 'NN')]\n",
      "[('eats', 'NNS')]\n",
      "[(' ', 'NN')]\n",
      "[('banana', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "reg_exp = RegexpTokenizer('(?u)\\W+|\\$[\\d\\.]+|\\S+')\n",
    "tokens1 = reg_exp.tokenize(\"Jerry eats banana\")\n",
    "for t in tokens1:\n",
    "    print(nltk.pos_tag([t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e6e0368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/cirrusrays/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Machine',\n",
       " 'learning',\n",
       " 'branch',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " '(',\n",
       " 'AI',\n",
       " ')',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'focuses',\n",
       " 'use',\n",
       " 'data',\n",
       " 'algorithms',\n",
       " 'imitate',\n",
       " 'way',\n",
       " 'humans',\n",
       " 'learn',\n",
       " ',',\n",
       " 'gradually',\n",
       " 'improving',\n",
       " 'accuracy',\n",
       " '.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words(\"english\")\n",
    "filtered_words = [t for t in tokens if t not in stop_words]\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b431b311",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4746e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous  ---> gener\n",
      "generate  ---> gener\n",
      "genorously  ---> genor\n",
      "generation  ---> gener\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "port = PorterStemmer()\n",
    "words = ['generous', 'generate','genorously', 'generation']\n",
    "for w in words:\n",
    "    print(w,\" --->\",port.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78067c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous  ---> generous\n",
      "generate  ---> generat\n",
      "genorously  ---> genor\n",
      "generation  ---> generat\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "words = ['generous', 'generate','genorously', 'generation']\n",
    "for w in words:\n",
    "    print(w,\" --->\",snowball.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f86968eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous  ---> gen\n",
      "generate  ---> gen\n",
      "genorously  ---> gen\n",
      "generation  ---> gen\n"
     ]
    }
   ],
   "source": [
    "# excessive Stemming\n",
    "from nltk.stem import LancasterStemmer\n",
    "lan = LancasterStemmer()\n",
    "words = ['generous', 'generate','genorously', 'generation']\n",
    "for w in words:\n",
    "    print(w,\" --->\",lan.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d4578d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous  ---> generou\n",
      "generate  ---> generate\n",
      "genorously  ---> genorou\n",
      "generation  ---> generat\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "reg = RegexpStemmer(\"s|ly|ion\")\n",
    "words = ['generous', 'generate','genorously', 'generation']\n",
    "for w in words:\n",
    "    print(w,\" --->\",reg.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a5689a",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a48bcef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eats  ---> eats\n",
      "ate  ---> ate\n",
      "eating  ---> eating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/cirrusrays/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemma = WordNetLemmatizer()\n",
    "words = ['eats', 'ate', 'eating']\n",
    "for w in words:\n",
    "    print(w,\" --->\",lemma.lemmatize(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eba15f8",
   "metadata": {},
   "source": [
    "### Word Sense Disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e329c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('metrical_foot.n.01') (prosody) a group of 2 or 3 syllables forming the basic unit of poetic rhythm\n",
      "Synset('foot.n.07') travel by walking\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "\n",
    "a1 = lesk(nltk.word_tokenize(\"The buiding is 100 feet height\"), \"feet\")\n",
    "print(a1,a1.definition())\n",
    "a2 = lesk(nltk.word_tokenize(\"I feet is paining after walking\"), \"feet\")\n",
    "print(a2,a2.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df29b6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('lie.n.02') Norwegian diplomat who was the first Secretary General of the United Nations (1896-1968)\n",
      "Synset('lie_down.v.01') assume a reclining position\n"
     ]
    }
   ],
   "source": [
    "a1 = lesk(nltk.word_tokenize(\"I was asked to lie on the bed\"), \"lie\")\n",
    "print(a1,a1.definition())\n",
    "a2 = lesk(nltk.word_tokenize(\"The boy was telling a lie\"), \"lie\")\n",
    "print(a2,a2.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ae8481",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4d773dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/cirrusrays/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/cirrusrays/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9676b25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON   ('Winston', 'NNP')\n",
      "PERSON   ('Dcosta', 'NNP')\n",
      "ORGANIZATION   ('Cirrusrays', 'NNP')\n"
     ]
    }
   ],
   "source": [
    "text = \"Winston Dcosta is an AI engineer working at Cirrusrays\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "for chunck in nltk.ne_chunk(nltk.pos_tag(tokens)):\n",
    "    if hasattr(chunck,\"label\"):\n",
    "        print(chunck.label(),\" \",chunck[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d9fab8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON   ('Narendra', 'NNP')\n",
      "ORGANIZATION   ('Modi', 'NNP')\n",
      "GPE   ('India', 'NNP')\n"
     ]
    }
   ],
   "source": [
    "text = \"Narendra Modi is the Prime Minister of India\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "for chunck in nltk.ne_chunk(nltk.pos_tag(tokens)):\n",
    "    if hasattr(chunck,\"label\"):\n",
    "        print(chunck.label(),\" \",chunck[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07b193d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8393d7e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
